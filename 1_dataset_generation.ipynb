{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de2629c2",
   "metadata": {},
   "source": [
    "#Â Dataset Generation\n",
    "\n",
    "The workshop contains three different notebooks. Each one focuses on a different stage:\n",
    "    \n",
    "1. Dataset Generation. The first notebook (this one) focuses on generating a dataset for training the model. We will create a Robust Test Suite to check that the dataset generated meets certain conditions\n",
    "2. Model Training. The second notebook focuses on training the model. We will create a Robust Test Suite to check that the trained model meets certain conditions.\n",
    "3. Model Inference. In the last notebook, we use mercury.monitoring to monitor data drift and estimate the predicted performance of the model without having the labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5206a7a0",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "You can install mercury-robust by running:\n",
    "\n",
    "```\n",
    "!pip install mercury-robust\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b58863",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(seed=2021)\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662ae305",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "\n",
    "We will use the default credit card Dataset from the UCI machine learning repository. The dataset was used in [[1]](#[1]). Note that we will use a slightly modified version which contains a time column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22503c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/uci_credit_drifted_historic.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfa1640",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cbec40",
   "metadata": {},
   "source": [
    "## Prepare Dataset For Training\n",
    "\n",
    "Let's preparing the dataset for training the model. Our label will be the \"default.payment.next.month\" variable. We select the features that we want to use in our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b345614",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 'default.payment.next.month'\n",
    "features = [c for c in df.columns if c not in [label, 'time', 'id']]\n",
    "#features = [c for c in df.columns if c not in [label, 'time', 'id', 'WARNING_SENT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69aeca82",
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce54b098",
   "metadata": {},
   "source": [
    "Now, let's define the function that will generate a train and test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c99c776",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def prepare_dataset(df, features, label, test_size=0.3, random_state=42):\n",
    "    \n",
    "    # Only Keep Features and label\n",
    "    df = df[features + [label]]\n",
    "    \n",
    "    # Drop Duplicates\n",
    "    #df = df.drop_duplicates()\n",
    "    \n",
    "    # Split Train/Test\n",
    "    df_train, df_test = train_test_split(df, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    return df, df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f991c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df, df_train, df_test = prepare_dataset(df, features, label, test_size=0.3, random_state=SEED)\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287ae8b5",
   "metadata": {},
   "source": [
    "## Create Data Schema\n",
    "\n",
    "We now use [mercury.dataschema](https://bbva.github.io/mercury-dataschema/) to create a `DataSchema` which contains the feature types of the dataset. This will be used later when creating the Robust Tests. \n",
    "\n",
    "The [`DataSchema`](https://bbva.github.io/mercury-dataschema/reference/dataschema/#mercury.dataschema.schemagen.DataSchema.generate) auto-infers the feature types, but we can also specify some feature types in case that the auto-inference doesn't work exactly as we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62379deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mercury.dataschema import DataSchema\n",
    "from mercury.dataschema.feature import FeatType\n",
    "\n",
    "custom_feature_mapping = {\n",
    "    \"PAY_0\": FeatType.DISCRETE,\n",
    "    \"PAY_2\": FeatType.DISCRETE,\n",
    "    \"PAY_3\": FeatType.DISCRETE,\n",
    "    \"PAY_4\": FeatType.DISCRETE,\n",
    "    \"PAY_5\": FeatType.DISCRETE,\n",
    "    \"PAY_6\": FeatType.DISCRETE,\n",
    "}\n",
    "\n",
    "schema = DataSchema().generate(df_train, force_types=custom_feature_mapping).calculate_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f48e934",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema.feats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f8645a",
   "metadata": {},
   "source": [
    "## Data Robust Tests\n",
    "\n",
    "We now [mercury.robust](https://bbva.github.io/mercury-robust/) to create tests to check that the generated dataset meets certain conditions.\n",
    "\n",
    "More concretely, we will create the next tests:\n",
    "1. [LinearCombinationsTest](https://bbva.github.io/mercury-robust/reference/data_tests/#mercury.robust.data_tests.LinearCombinationsTest): Ensures that the dataset doesn't have any linear combination between its numerical columns and no categorical variable is redundant\n",
    "2. [LabelLeakingTest](https://bbva.github.io/mercury-robust/reference/data_tests/#mercury.robust.data_tests.LabelLeakingTest): Ensures the target variable is not being leaked into the predictors.\n",
    "3. [NoisyLabelTest](https://bbva.github.io/mercury-robust/reference/data_tests/#mercury.robust.data_tests.NoisyLabelsTest): Looks if the labels of a dataset contain a high level of noise.\n",
    "4. [SampleLeakingTest](https://bbva.github.io/mercury-robust/reference/data_tests/#mercury.robust.data_tests.SampleLeakingTest): Looks if there are samples in the test dataset that are identical to samples in the base/train dataset.\n",
    "5. [NoDuplicatesTest](https://bbva.github.io/mercury-robust/reference/data_tests/#mercury.robust.data_tests.NoDuplicatesTest): Checks no duplicated samples are present in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408b3fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mercury.robust.data_tests import (\n",
    "    LinearCombinationsTest,\n",
    "    LabelLeakingTest,\n",
    "    NoisyLabelsTest,\n",
    "    SampleLeakingTest,\n",
    "    NoDuplicatesTest\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0137546",
   "metadata": {},
   "source": [
    "We have two options to execute the tests: We can just execute one test individually, or alternatively, run a group of test in a `TestSuite`.\n",
    "\n",
    "Let's start running an individual test with the [`LinearCombinationsTest`](https://bbva.github.io/mercury-robust/reference/data_tests/#mercury.robust.data_tests.LinearCombinationsTest):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63690468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LinearCombinationsTest\n",
    "linear_combinations = LinearCombinationsTest(df[features], dataset_schema=schema)\n",
    "linear_combinations.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f130fd",
   "metadata": {},
   "source": [
    "When no exception is raised, the test has run successfully. Let's try another test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f87cf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LabelLeakingTest\n",
    "label_leaking = LabelLeakingTest(\n",
    "    df[features + [label]], \n",
    "    label_name = label,\n",
    "    task = \"classification\",\n",
    "    dataset_schema=schema,\n",
    ")\n",
    "label_leaking.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c93939f",
   "metadata": {},
   "source": [
    "Now the test has failed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5d0881",
   "metadata": {},
   "source": [
    "## Test Suite\n",
    "\n",
    "Now we will group several test in a `TestSuite` and execute them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435142d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mercury.robust import TestSuite\n",
    "\n",
    "def create_suite(df, df_train, df_test, schema, features, label):\n",
    "\n",
    "    # LinearCombinationsTest\n",
    "    linear_combinations = LinearCombinationsTest(df[features], dataset_schema=schema)\n",
    "    \n",
    "    # LabelLeakingTest\n",
    "    label_leaking = LabelLeakingTest(\n",
    "        df[features + [label]], \n",
    "        label_name = label,\n",
    "        task = \"classification\",\n",
    "        dataset_schema=schema,\n",
    "    )\n",
    "    \n",
    "    # Noisy Labels\n",
    "    noisy_labels = NoisyLabelsTest(\n",
    "        base_dataset=df[features + [label]],\n",
    "        label_name=label,\n",
    "        calculate_idx_issues=True,\n",
    "        threshold = 0.2,\n",
    "        dataset_schema=schema,\n",
    "        label_issues_args={\"clf\": LogisticRegression(solver='liblinear')}\n",
    "    )\n",
    "    \n",
    "    # SampleLeaking\n",
    "    sample_leaking = SampleLeakingTest(\n",
    "        base_dataset=df_train[features + [label]], \n",
    "        test_dataset=df_test[features + [label]]\n",
    "    )\n",
    "    \n",
    "    # NoDuplicates\n",
    "    no_dups = NoDuplicatesTest(df_train)\n",
    "    \n",
    "    # Create Suite\n",
    "    test_suite = TestSuite(\n",
    "        tests=[\n",
    "            linear_combinations,\n",
    "            label_leaking,\n",
    "            noisy_labels,\n",
    "            sample_leaking,\n",
    "            no_dups\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return test_suite\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5ca489",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_suite = create_suite(df, df_train, df_test, schema, features, label)\n",
    "test_results = test_suite.run()\n",
    "test_suite.get_results_as_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5267e1ac",
   "metadata": {},
   "source": [
    "## Save Dataset and Data Schema\n",
    "\n",
    "Let's save our generated dataset and the `DataSchema`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27e2d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dataset = \"./dataset/\"\n",
    "\n",
    "if not os.path.exists(path_dataset):\n",
    "    os.makedirs(path_dataset)\n",
    "\n",
    "df.to_csv(path_dataset + \"all.csv\", index=False)\n",
    "df_train.to_csv(path_dataset + \"train.csv\", index=False)\n",
    "df_test.to_csv(path_dataset + \"test.csv\", index=False)\n",
    "\n",
    "schema.save(path_dataset + \"schema.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724efe8a",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "<a id=\"[1]\">[1]</a>\n",
    "Yeh, I. C., & Lien, C. H. (2009). The comparisons of data mining techniques for the predictive accuracy of probability of default of credit card clients. Expert Systems with Applications, 36(2), 2473-2480. https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b2d202",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
