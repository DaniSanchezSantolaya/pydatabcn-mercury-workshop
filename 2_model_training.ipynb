{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a4baa4f",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "The workshop contains three different notebooks. Each one focuses on a different stage:\n",
    "    \n",
    "1. Dataset Generation. The first notebook focuses on generating a dataset for training the model. We will create a Robust Test Suite to check that the dataset generated meets certain conditions\n",
    "2. Model Training (This Notebook). The second notebook focuses on training the model. We will create a Robust Test Suite to check that the trained model meets certain conditions.\n",
    "3. Model Inference. In the last notebook, we use mercury.monitoring to monitor data drift and estimate the predicted performance of the model without having the labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49992cc",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "You can install mercury-robust by running:\n",
    "\n",
    "```\n",
    "!pip install mercury-robust\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed813b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score, precision_score\n",
    "\n",
    "SEED = 23\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30e59ee",
   "metadata": {},
   "source": [
    "## Read Dataset\n",
    "\n",
    "Let's read the dataset that we generated in the first notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2884711",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dataset = \"./dataset/\"\n",
    "\n",
    "df_train = pd.read_csv(path_dataset + \"train.csv\")\n",
    "df_test = pd.read_csv(path_dataset + \"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540fe21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eee3356",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "\n",
    "Let's train a Decision Tree to predict if a client will default its credit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba483cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = \"default.payment.next.month\"\n",
    "features = [c for c in df_train.columns if c!=label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f71847",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train[features]\n",
    "y_train = df_train[label]\n",
    "\n",
    "X_test = df_test[features]\n",
    "y_test = df_test[label]\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=SEED)\n",
    "#model = DecisionTreeClassifier(\n",
    "#    max_depth=6, class_weight=\"balanced\", min_samples_split=15, min_samples_leaf=15, random_state=SEED\n",
    "#)\n",
    "model = model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4b2e9c",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba40bd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_test = accuracy_score(y_test, model.predict(X_test))\n",
    "auc_test = roc_auc_score(y_test, model.predict_proba(X_test)[:,1])\n",
    "f1_score_test = f1_score(y_test, model.predict(X_test))\n",
    "\n",
    "print(\"accuracy: \", acc_test)\n",
    "print(\"AUC: \", auc_test)\n",
    "print(\"F1: \", f1_score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349a114f",
   "metadata": {},
   "source": [
    "## Robust Model Test Suite\n",
    "\n",
    "As we did when creating the dataset, we will create a `TestSuite` using [mercury.robust](https://bbva.github.io/mercury-robust/). This time, we will focus on testing the trained model creating the next tests:\n",
    "\n",
    "- [ModelSimplicityChecker](https://bbva.github.io/mercury-robust/reference/model_tests/#mercury.robust.model_tests.ModelSimplicityChecker): Looks if a trained model has a simple baseline which trained in the same dataset gives better or similar performance on a test dataset\n",
    "- [CohortPerformanceTest](https://bbva.github.io/mercury-robust/reference/data_tests/#mercury.robust.data_tests.CohortPerformanceTest): looks if some metric performs poorly for some cohort of your data when compared with other groups\n",
    "- [DriftMetricResistanceTest](https://bbva.github.io/mercury-robust/reference/model_tests/#mercury.robust.model_tests.DriftMetricResistanceTest): Checks the robustness of a trained model to drift in the inputs of the data.\n",
    "- [TreeCoverageTest](https://bbva.github.io/mercury-robust/reference/model_tests/#mercury.robust.model_tests.TreeCoverageTest): Checks whether a given test_dataset covers a minimum of all the branches of a tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82f0572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data Schema\n",
    "from mercury.dataschema import DataSchema\n",
    "schema = DataSchema.load(path_dataset + \"schema.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e462192",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mercury.robust.model_tests import (\n",
    "    ModelSimplicityChecker,\n",
    "    DriftMetricResistanceTest,\n",
    "    TreeCoverageTest\n",
    ")\n",
    "from mercury.robust.data_tests import CohortPerformanceTest\n",
    "from mercury.robust import TestSuite\n",
    "\n",
    "def create_model_test_suite(\n",
    "    model, \n",
    "    X_train, \n",
    "    y_train,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    schema,\n",
    "    add_tree_coverage_test=False\n",
    "):\n",
    "    \n",
    "    model_tests = []\n",
    "    \n",
    "    # Model Simpclicity Checker\n",
    "    model_simplicity_checker = ModelSimplicityChecker(\n",
    "        model = model,\n",
    "        X_train = X_train,\n",
    "        y_train = y_train,\n",
    "        X_test = X_test,\n",
    "        y_test = y_test,\n",
    "        threshold = 0.02,\n",
    "        eval_fn = roc_auc_score,\n",
    "        ignore_feats=label,\n",
    "        dataset_schema=schema,\n",
    "        baseline_model=LogisticRegression(solver='liblinear', class_weight='balanced')\n",
    "    )\n",
    "    model_tests.append(model_simplicity_checker)\n",
    "    \n",
    "    # Cohort Performance Test\n",
    "    group = \"SEX\"\n",
    "    def eval_precision(df):\n",
    "        return precision_score(df[label], df[\"prediction\"])\n",
    "\n",
    "    # Calculate predictions, we will use this in one test\n",
    "    df_test_pred = pd.DataFrame()\n",
    "    df_test_pred[group] = X_test[group].values\n",
    "    df_test_pred[\"prediction\"] = model.predict(X_test)\n",
    "    df_test_pred[label] = y_test\n",
    "    cohort_perf_test = CohortPerformanceTest(\n",
    "        name=\"precision_by_gender_check\",\n",
    "        base_dataset=df_test_pred, group_col=\"SEX\", eval_fn = eval_precision, threshold = 0.05,\n",
    "        threshold_is_percentage=False\n",
    "    )\n",
    "    model_tests.append(cohort_perf_test)\n",
    "    \n",
    "    # One DriftMetricResistanceTest for each variable\n",
    "    for f in features:\n",
    "        drift_args = None\n",
    "        if ('BILL_AMT' in f) or ('PAY_AMT' in f):\n",
    "            drift_args = {'cols': [f], 'force': df_train[f].quantile(q=0.25)}\n",
    "        elif 'PAY_' in f:\n",
    "            drift_args = {'cols': [f], 'force': 2}\n",
    "        if drift_args is not None:\n",
    "            model_tests.append(DriftMetricResistanceTest(\n",
    "                model = model,\n",
    "                X = X_test,\n",
    "                Y = y_test,\n",
    "                drift_type = 'shift_drift',\n",
    "                drift_args = drift_args,\n",
    "                tolerance = 0.05,\n",
    "                eval=accuracy_score,\n",
    "                name=\"drift resistance \" + f\n",
    "            ))\n",
    "        \n",
    "    #Â Tree Coverage Test(only if specified)\n",
    "    if add_tree_coverage_test:\n",
    "        tree_coverage_test = TreeCoverageTest(model, X_test, threshold_coverage=.75)\n",
    "        model_tests.append(tree_coverage_test)\n",
    "    \n",
    "    # Create Suite\n",
    "    test_suite = TestSuite(\n",
    "        tests=model_tests\n",
    "    )\n",
    "    \n",
    "    return test_suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924427e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_suite = create_model_test_suite(\n",
    "    model, \n",
    "    X_train, \n",
    "    y_train,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    schema,\n",
    "    add_tree_coverage_test=True\n",
    ")\n",
    "test_results = test_suite.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927c3f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_suite.get_results_as_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9832d19",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c418365",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model = \"./models/\"\n",
    "\n",
    "if not os.path.exists(path_model):\n",
    "    os.makedirs(path_model)\n",
    "    \n",
    "from joblib import dump\n",
    "dump(model, path_model + 'model.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0785cadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(path_model + \"features.pkl\", \"wb\") as fp:\n",
    "    pickle.dump(features, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
